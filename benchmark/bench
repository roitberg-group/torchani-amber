#!/usr/bin/env python
from numpy.typing import NDArray
from scipy import stats
from collections import defaultdict
import matplotlib.pyplot as plt
import numpy as np
import hashlib
from copy import deepcopy
import os
import sys
import shutil
import subprocess
import uuid
import json
import typing_extensions as tpx
from typer import Option
import itertools
from enum import Enum
import typing as tp
import dataclasses
from pathlib import Path

from rich.console import Console
from typer import Typer, Abort
import jinja2

console = Console()
app = Typer()

this_dir = Path(__file__).parent
systems_dir = this_dir / "systems"

# Time conversion factors
ms_to_s = 1 / 1000
s_to_hr = 1 / 3600
s_to_min = 1 / 60


@dataclasses.dataclass
class BenchMlmmConfig:
    protocol: str = "ani_me"
    use_torch_coupling: bool = True
    cuda: bool = True
    selection: str = ":1"  # ML system selection, assumed to be of the form ':N-M'
    float64: bool = True
    use_cuaev: bool = False
    cutoff: float = 15.0


class Protocol(Enum):
    ANI_ME = "ani_me"
    SANDER_ME = "sander_me"
    ANI_MBISPOL = "ani_mbispol"


@dataclasses.dataclass
class BenchConfig:
    system: str = "ala-8-chain"
    core_num: int = 1
    mlmm: tp.Optional[BenchMlmmConfig] = None

    @property
    def shorthash(self) -> str:
        self_dict = dataclasses.asdict(self)
        self_dict.pop("core_num")
        if self.mlmm is not None:
            self_dict["mlmm"].pop("selection")
        hasher = hashlib.shake_128()
        hasher.update(json.dumps(self_dict).encode("utf-8"))
        return hasher.hexdigest(8)

    @property
    def id(self) -> tp.Tuple[str, int, int]:
        return (self.shorthash, self.core_num, self.ml_residue_num)

    @property
    def label(self) -> str:
        parts = []
        if self.mlmm is None:
            return "Full MM"
        if self.mlmm.protocol == "ani_me":
            parts.append("ME")
        elif self.mlmm.protocol == "sander_me":
            parts.append("Sander ME")
        elif self.mlmm.protocol == "ani_mbispol":
            parts.append("MBIS + Pol")

        if self.mlmm.float64:
            parts.append("F64")
        if self.mlmm.use_torch_coupling:
            parts.append("Torch-accel")

        parts.append("CUDA" if self.mlmm.cuda else "CPU")
        if self.mlmm.use_cuaev:
            parts.append("cuAEV")
        parts.append(f"cutoff: {self.mlmm.cutoff:.1f}")
        return ", ".join(parts)

    @property
    def style(self) -> tp.Dict[str, tp.Any]:
        if self.mlmm is None:
            return {
                "color": "tab:blue",
                "linestyle": "dashed",
                "alpha": 1.0,
                "marker": "o",
            }
        if self.mlmm.protocol == "ani_me":
            c = "tab:orange" if self.mlmm.use_cuaev else "orangered"
        elif self.mlmm.protocol == "sander_me":
            c = "tab:red" if self.mlmm.use_cuaev else "darkred"
        elif self.mlmm.protocol == "ani_mbispol":
            c = "tab:purple" if self.mlmm.use_cuaev else "deeppink"
        if self.mlmm.float64:
            s = "dashed" if self.mlmm.cuda else "dotted"
        else:
            s = "dashdot" if self.mlmm.cuda else (0, (3, 1, 1, 1, 1, 1))  # type: ignore

        if self.mlmm.use_torch_coupling:
            a = 0.8
            if self.mlmm.cutoff == 15.0:
                m = "^"
            elif self.mlmm.cutoff == 11.5:
                m = "*"
            elif self.mlmm.cutoff == 8.0:
                m = "X"
        else:
            a = 1.0
            if self.mlmm.cutoff == 15.0:
                m = "o"
            elif self.mlmm.cutoff == 11.5:
                m = "d"
            elif self.mlmm.cutoff == 8.0:
                m = "x"
        return {"color": c, "linestyle": s, "alpha": a, "marker": m}

    @property
    def ml_residue_num(self) -> int:
        if self.mlmm is None:
            return 0
        s = self.mlmm.selection.replace(":", "")
        if len(s) == 1:
            return 1
        lo, hi = s.split("-")
        return 1 + int(hi) - int(lo)

    def to_file(self, path: Path) -> None:
        with open(path, mode="wt", encoding="utf-8") as f:
            json.dump(dataclasses.asdict(self), f, indent=4)
            # Ensure data is written to files
            f.flush()
            os.fsync(f.fileno())


@app.command()
def run(
    slurm: tpx.Annotated[
        bool,
        Option(
            "--slurm/--no-slurm",
            help="Generate a sbatch file and execute it (specific for HPG)",
        ),
    ] = False,
    system: tpx.Annotated[
        str,
        Option(
            "--system",
            help="Name of system to run on. Must be found in 'benchmark/systems'",
        ),
    ] = "ala-8-chain",
    cutoff: tpx.Annotated[
        float,
        Option("--cutoff", help="Cutoff of QM part to use"),
    ] = 15.0,
    float64: tpx.Annotated[
        bool,
        Option("--f64/--no-f64", help="Benchmark float64"),
    ] = True,
    torchcouple: tpx.Annotated[
        bool,
        Option("--torchcouple/--no-torchcouple", help="Benchmark torch coupling"),
    ] = True,
    skip_slow: tpx.Annotated[
        bool,
        Option("--skip-slow/--no-skip-slow", help="Skip unused slow algorithm"),
    ] = True,
    cuaev: tpx.Annotated[
        bool,
        Option("--cuaev/--no-cuaev", help="Benchmark cuaev"),
    ] = False,
    cuda: tpx.Annotated[
        bool,
        Option("--cuda/--no-cuda", help="Benchmark on CUDA device"),
    ] = True,
    core_num: tpx.Annotated[
        int,
        Option(
            "-c",
            "--cores",
            help="Number of cores to run benchmark on, if > 1 sander.MPI is used",
        ),
    ] = 1,
    # At least 200 are needed
    tstep_num: tpx.Annotated[
        int,
        Option(
            "-t",
            "--tsteps",
            help="Number of timesteps to run benchmarks for",
        ),
    ] = 500,
    bench_dir: tpx.Annotated[
        tp.Optional[Path],
        Option(
            "-b",
            "--bench-dir",
            show_default=False,
            help="Full path to benchmark directory."
            " Created automatically by default, so no need to use this opt in general",
        ),
    ] = None,
    selections: tpx.Annotated[
        tp.Optional[tp.List[str]],
        Option(
            "-s",
            "--selection",
            help="'Ambermask' that determines which part of the system is QM."
            " If specified multiple times multiple configs are run"
            " each with a different selection",
        ),
    ] = None,
    verbose: tpx.Annotated[
        bool,
        Option("-v/-V", "--verbose/--no-verbose"),
    ] = True,
    suffix: tpx.Annotated[
        str,
        Option("--suffix", help="Suffix for the benchmark name"),
    ] = "",
) -> None:
    r"""Run Sander or Sander-MPI ML/MM benchmarks"""
    if bench_dir is None:
        kind = "serial" if core_num == 1 else "mpi"
        dirname = f"{kind}-bench-{str(uuid.uuid4()).split('-')[0]}"
        if suffix:
            dirname = f"{dirname}-{suffix}"
        bench_dir = this_dir / dirname
        bench_dir.mkdir()

    env = jinja2.Environment(
        loader=jinja2.FileSystemLoader(Path(__file__).parent / "templates/"),
        undefined=jinja2.StrictUndefined,
        autoescape=jinja2.select_autoescape(),
        trim_blocks=True,
        lstrip_blocks=True,
    )
    if slurm:
        arg_list = sys.argv[1:]
        for j, arg in enumerate(deepcopy(arg_list)):
            if arg in ["-s", "--selection", "--system"]:
                # re-introduce quotes in the selections and system
                arg_list[j + 1] = f"'{arg_list[j + 1]}'"
        args = " ".join(arg_list)
        args = args.replace(" --slurm", "")
        tmpl = env.get_template("bench.slurm.sh.jinja").render(
            cli_app_dir=str(this_dir),
            core_num=core_num,
            args=args,
            bench_dir=str(bench_dir),
        )
        input_fpath = bench_dir / "hpg.slurm.sh"
        input_fpath.write_text(tmpl)
        console.print("Launching slurm script ...")
        subprocess.run(["sbatch", str(input_fpath)], cwd=bench_dir, check=True)
        sys.exit(0)

    protos = [Protocol.ANI_ME, Protocol.SANDER_ME, Protocol.ANI_MBISPOL]
    if selections is None:
        selections = [
            ":1",
            ":1-2",
            ":1-3",
            ":1-4",
            ":1-5",
            ":1-6",
            ":1-7",
            ":1-8",
            ":1-9",
        ]
    # Base, "MM/MM" config
    configs = [BenchConfig(core_num=core_num, system=system)]
    if cuaev and not cuda:
        console.print("cuAEV needs CUDA device")
        raise Abort()
    for proto, selection in itertools.product(
        protos,
        selections,
    ):
        # Discard incompatible configs
        if torchcouple and proto == Protocol.SANDER_ME:
            continue
        if skip_slow and not torchcouple and proto == Protocol.ANI_MBISPOL:
            continue
        configs.append(
            BenchConfig(
                core_num=core_num,
                system=system,
                mlmm=BenchMlmmConfig(
                    proto.value,
                    use_torch_coupling=torchcouple,
                    cuda=cuda,
                    selection=selection,
                    float64=float64,
                    use_cuaev=cuaev,
                    cutoff=cutoff,
                ),
            )
        )

    if verbose:
        console.print(
            f"Will use {core_num} cores ({'serial' if core_num == 1 else 'mpi'})"
        )
        console.print(f"Will benchmark {len(configs)} configs total")
        console.print(f"Will run each config for {tstep_num} steps")
        estimate_ms = len(configs) * 800 * tstep_num / core_num
        if estimate_ms * ms_to_s * s_to_hr < 1:
            factor = s_to_min
            unit = "min"
        else:
            factor = s_to_hr
            unit = "hr"
        console.print(f"Time estimate {estimate_ms * ms_to_s * factor:.2f} {unit}")

    for config_idx, config in enumerate(configs):
        input_as_str = env.get_template("mlmm.mdin.jinja").render(
            tstep_num=tstep_num, **dataclasses.asdict(config)
        )
        config_dir = bench_dir / str(uuid.uuid4()).split("-")[0]
        config_dir.mkdir()
        shutil.copy(systems_dir / f"{config.system}.inpcrd", config_dir / "inpcrd")
        shutil.copy(systems_dir / f"{config.system}.prmtop", config_dir / "prmtop")
        (config_dir / "mdin").write_text(input_as_str)
        cmd = (
            ["sander"]
            if config.core_num == 1
            else ["mpirun", "-n", str(config.core_num), "sander.MPI"]
        )
        if verbose:
            console.print(f"{config_idx + 1}/{len(configs)} Running {config}")
        # This requires a version of amber that is hooked with this function
        os.environ["TORCHANI_AMBER_WALLTIME"] = "1"
        subprocess.run(cmd, cwd=config_dir, check=True)
        config.to_file(config_dir / "config.json")


@app.command()
def plot(
    system: tpx.Annotated[
        str,
        Option("-s", "--system"),
    ] = "ala-8-chain",
    mpi_scaling: tpx.Annotated[
        int,
        Option("--mpi-scaling"),
    ] = False,
    size_scaling: tpx.Annotated[
        bool,
        Option("--size-scaling"),
    ] = True,
    core_num: tpx.Annotated[
        int,
        Option("-c", "--cores", help="Only for size scaling"),
    ] = 1,
    ml_residue_num: tpx.Annotated[
        int,
        Option("-r", "--res", help="Only for cores scaling"),
    ] = 9,
    torchcouple: tpx.Annotated[
        tp.Optional[bool],
        Option("--torchcouple/--no-torchcouple", help="Torch coupling"),
    ] = None,
    cuda: tpx.Annotated[
        tp.Optional[bool],
        Option("--cuda/--no-cuda"),
    ] = None,
    cuaev: tpx.Annotated[
        tp.Optional[bool],
        Option("--cuaev/--no-cuaev"),
    ] = None,
    float64: tpx.Annotated[
        tp.Optional[bool],
        Option("--f64/--no-f64"),
    ] = None,
    warm_up: tpx.Annotated[
        int,
        Option("-w", "--warm-up"),
    ] = 100,
    ylo: tpx.Annotated[
        tp.Optional[float],
        Option("--ylo"),
    ] = None,
    yhi: tpx.Annotated[
        tp.Optional[float],
        Option("--yhi"),
    ] = None,
    err: tpx.Annotated[
        str,
        Option("--err"),
    ] = "sem",
    cutoff: tpx.Annotated[
        tp.Optional[float],
        Option("--cutoff", help="Cutoff of QM part to use"),
    ] = None,
    proto: tpx.Annotated[
        tp.Optional[str],
        Option("--proto", help="Cutoff of QM part to use"),
    ] = None,
    except_proto: tpx.Annotated[
        tp.Optional[str],
        Option("--except-proto", help="Cutoff of QM part to use"),
    ] = None,
    verbose: tpx.Annotated[
        bool,
        Option("-v/-V", "--verbose/--no-verbose"),
    ] = False,
) -> None:
    # Collect and filter configs
    results: tp.List[tp.Any] = []
    for d in this_dir.glob("*-bench-*"):
        for subd in d.iterdir():
            if not subd.is_dir():
                continue
            if not (subd / "config.json").is_file():
                console.print(f"Error in {subd}, can't finde config.json", style="red")
                continue
            with open(subd / "config.json", mode="rt", encoding="utf-8") as f:
                bench = json.load(f)
                if "mlmm" in bench:
                    mlmm = bench.pop("mlmm")
                else:
                    mlmm = None
                config = BenchConfig(**bench)
                config.mlmm = BenchMlmmConfig(**mlmm) if mlmm is not None else None
            if torchcouple is not None:
                if config.mlmm and config.mlmm.use_torch_coupling != torchcouple:
                    continue
            if proto is not None:
                if config.mlmm and config.mlmm.protocol != proto:
                    continue
            if except_proto is not None:
                if config.mlmm and config.mlmm.protocol == except_proto:
                    continue
            if cutoff is not None:
                if config.mlmm and config.mlmm.cutoff != cutoff:
                    continue
            if cuaev is not None:
                if config.mlmm and config.mlmm.use_cuaev != cuaev:
                    continue
            if float64 is not None:
                if config.mlmm and config.mlmm.float64 != float64:
                    continue
            if cuda is not None:
                if config.mlmm and config.mlmm.cuda != cuda:
                    continue
            if config.system != system:
                continue
            if size_scaling and config.core_num != core_num:
                continue
            if mpi_scaling:
                if config.ml_residue_num != ml_residue_num:
                    continue

            if not (subd / "walltime_ms.dat").is_file():
                console.print(
                    f"Error in {subd}, can't find walltime_ms.dat", style="red"
                )
                continue
            walltimes = np.array(
                [float(t) for t in (subd / "walltime_ms.dat").read_text().split()]
            )
            results.append({"config": config, "walltime_ms": walltimes})
            if verbose:
                console.print(subd)
                console.print(config)
                console.print(len(walltimes))

    if not results:
        console.print(
            "No results were found with the requested constraints", style="yellow"
        )
        sys.exit(0)

    if size_scaling:
        fig, ax = plt.subplots()
        ax.set_xlabel(r"Num. ML residues")
        ax.set_ylabel(r"Mean walltime per timestep (ms)")
        scaling_data: tp.Dict[str, tp.Dict[int, NDArray[np.float64]]] = defaultdict(
            dict
        )
        styles = {}

        # TODO: replicas
        for r in results:
            config = r["config"]
            walltimes = r["walltime_ms"]
            num_res = config.ml_residue_num
            scaling_data[r["config"].label][num_res] = walltimes[warm_up:]
            styles[r["config"].label] = r["config"].style

        all_res_nums = set()
        for label, xy in scaling_data.items():
            style = styles[label]
            marker = style.pop("marker")
            ml_res_nums = np.array(list(xy.keys()))
            all_res_nums.update(ml_res_nums.tolist())
            idxs = np.argsort(ml_res_nums)
            ml_res_nums = ml_res_nums[idxs]
            mean_wts = np.array([np.mean(wts).item() for wts in xy.values()])[idxs]
            if err != "no":
                if err == "sem":
                    errs = np.array([stats.sem(wts).item() for wts in xy.values()])[
                        idxs
                    ]
                elif err == "std":
                    errs = np.array([np.std(wts).item() for wts in xy.values()])[idxs]
                ax.errorbar(ml_res_nums, mean_wts, yerr=errs, **style)
            else:
                ax.plot(ml_res_nums, mean_wts, **style)
            ax.scatter(
                ml_res_nums,
                mean_wts,
                label=label,
                s=7.0,
                color=style["color"],
                alpha=style["alpha"],
                marker=marker,
            )
            if len(ml_res_nums) == 1 and ml_res_nums[0] == 0:
                ax.axhline(
                    y=mean_wts[0],
                    **style,
                )
        ax.set_ylim(ylo, yhi)
        ax.set_xticks(sorted(all_res_nums))
        ax.set_title(
            (
                f"MPI benchmark with {core_num} cores"
                if core_num > 1
                else "Serial benchmark"
            )
            + f" on {system}"
        )
        ax.legend()
        plt.show()


if __name__ == "__main__":
    app()
